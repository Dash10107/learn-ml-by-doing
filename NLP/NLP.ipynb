{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOlWkxvSId3Hy04GxdxgxmF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFeCggKcFsfh","executionInfo":{"status":"ok","timestamp":1720606082717,"user_tz":-330,"elapsed":5552,"user":{"displayName":"Daksh Jain","userId":"16209101031814765558"}},"outputId":"3a8e186f-367f-4340-e9c8-53c31e93b77a"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n","[[1, 2, 3, 4], [1, 2, 3, 5]]\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentence=[\n","    \"I love my dog\",\n","    \"I love my cat\"\n","]\n","\n","tokenizer=Tokenizer(num_words=100)\n","tokenizer.fit_on_texts(sentence)\n","word_index=tokenizer.word_index\n","print(word_index)\n","\n","sequences=tokenizer.texts_to_sequences(sentence)\n","print(sequences)"]}]}